---
title: "Lab Journal - Gene Expression Analysis"
subtitle: "Noise exposures causing hearing loss generate proteotoxic stress and activate the proteostasis network"
output:
  pdf_document

header-includes:
- \pagenumbering{gobble}
---


<!-- (Front page) -->
\vspace{69pt}
\begin{center}
  \includegraphics[]{cover_image_mouse.jpeg}
\end{center}

\hfill \textbf{Student}: Vincent Talen  

\hfill \textbf{Student number}: 389015  

\hfill \textbf{Class}: BFV2

\hfill \textbf{Study}: Bio-Informatica

\hfill \textbf{Institute}: Institute for Life Science & Technology

\hfill \textbf{Teacher}: Marcel Kempenaar

\hfill \textbf{Date}: `r Sys.Date()`


<!-- (Page 01 - Table of contents) -->
\newpage
\setcounter{secnumdepth}{2}
\tableofcontents
\pagenumbering{arabic}


<!-- (Page 02 - R Setup) -->
\newpage
# R Setup
To correctly create this document and to be able to run all the R code a few settings have to be set and libraries imported. All the code chunks need to be visible in the end pdf so the echo is set to true and to render the pdf's faster whilst developing the cache is also set to true. Multiple packages are used in this project, all of them are placed in a vector which then can be used to load all the packages on a single line with lapply. For faster rendering times when using DESeq2 a higher core amount it specified.

```{r setup, results="hide", message = FALSE, warning = FALSE}
# Set code chunks visibility in pdf output to true
knitr::opts_chunk$set(echo = TRUE)
# Use cache
knitr::opts_chunk$set(cache = TRUE)

# Create vector with all packages that will be used
packages <- c("affy", "pander", "scales", "BiocParallel", 
              "DESeq2", "pheatmap", "PoiClaClu", "ggplot2")
# Load each package in the vector with lapply
invisible(lapply(packages, library, character.only = TRUE))
# Drop the packages variable from memory since it will not be used again
remove(packages)
# Set cores to be used for when using Bioc DESeq2 
register(MulticoreParam(6))
```


<!-- (Page 03 - Preparing the data) -->
\newpage
# Exploratory Data Analysis
## Preparing the data
To start this project out the data set will be loaded into the memory so the statistics can be performed on it.
The first few lines will be shown to show what the contents of the data frame are like and dim() is used to show the length of the data set.

```{r loading in dataset}
# Load in the data set into a data frame
myData <- read.table("GSE160639_RawReadCount.txt", header = T, sep = "\t")
# Change the int row names to the gene_id column and delete the gene_id column afterwards
row.names(myData) <- myData$gene_id
myData <- myData[-1]
# Show the first few lines of the data frame
head(myData)

# Print the amount of genes and columns
cat("Amount of genes:", dim(myData)[1], "\tColumns in dataframe", dim(myData)[2])
```

By using the str() function the structure of the dataframe/dataset can be seen. Every column consists only of int values, so only the count data. (Resulting table not shown in PDF)

```{r check dataset structure, results="hide"}
str(myData)
```

For future use the `groups` variable is already created since it will be needed multiple times. It is used to define what group a sample belongs to.
```{r}
groups <- factor(rep(1:3, each=4), labels = c("70dB", "94dB", "105dB"))
```


<!-- (Page 04 - Data summary) -->
\newpage
## Data Summary
To get to know the data set more the data will be visualized in multiple ways. Seeing how everything is grouped and divided gives us a deeper understanding of what the quality of the data set is and what types of statistics will have to be performed.  

First a simple summary performing 6-number-statistic on the data columns will be done, this gives a summary overview of each replication for all groups.

```{r data frame summary}
# Disable printing 'table continues' lines between split sections of the table
panderOptions("table.continues", "")
# Pretty print the summary of the data frame
pander::pander(summary(myData), split.tables = 80)
```

At first glance of the summary it can be seen that the expression at the lowest (70) dB genes seem to be expressed the most with the expression declining whilst the SPL increases. It also looks like there is a noticeable difference between each replication.


<!-- (Page 05 - Boxplot) -->
\newpage
## Boxplot
Now a boxplot will be made for each data column, visualizing this will help quickly spot irregularities after which further detailed statistics need to be performed to do any quality control. To increase the visibility a log2 transformation will be done to show changes more informative.

```{r boxplot}
# Create a color pallet variable to use in graphs
myColors = rep(hue_pal()(3), each = 4)
# Plot the boxplot with log2(1) scale.
boxplot(log2(myData + 1), col = myColors,
        outline = FALSE,las = 2, ylab = "log2(Gene expression count)", 
        main="Log2(1) gene expression boxplot per sample")
``` 


<!-- (Page 06 - Density plot) -->
\newpage
## Density plot
Another useful way of visualizing the data set is using a density plot, it shows a distribution of the log2-transformed count data for all samples which makes spotting problems easier.

```{r density plot}
# Plot a density plot with log2(0.1) scale
# Use a different color for each group and line type for each replication
plotDensity(log2(myData + 0.1), col=myColors,
            lty=c(1:ncol(myData)), xlab="Log2(count)",
            main="Expression Distribution")

# Add a legend and a vertical line
legend("topright", names(myData), lty=c(1:ncol(myData)), col=myColors)
abline(v=-1.5, lwd=1, col="black", lty=2)
```

Looking at the density plot of the expression distribution it looks like all replications have a relatively similar amount of reads sequenced since all the peaks lie together.


<!-- (Page 07 - Checking read depth with barplot) -->
\newpage
## Normalizing the data
### Checking read depth with barplot
Sometimes there seems to be a difference in expression between groups even if there actually isn't, if there is a big difference in count values within the samples of a group that could be due to the read depth of the samples. To see the read depth of all samples fast, the total number of mapped reads per sample can be shown in a bar plot.

```{r read depth barplot}
# Initialize plot
barplot(colSums(myData) / 1e6, las = 2, border = NA)

# Add horizontal axis
axis(2, tck = 1, lty = 1, col = "gray", labels = FALSE)

# Create the plot
par(new = TRUE)
barplot(colSums(myData) / 1e6, main = "Read counts for GSE160639",
        las = 2, col = myColors, border = NA,
        ylab = "Sequencing depth (millions of reads)")
```

Looking at the bar plot that was created it can be seen that all the samples have a good, similar, sequencing depth. Though all the read depths are high and similar the data will still need to be normalized, because there might still be unwanted variation disrupting our analysis.


<!-- (Page 08 - Normalizing the data with a variance stabilizing transformation) -->
\newpage
### Performing the data normalization with a variance stabilizing transformation
To normalize the data the *vsc* function of the DESeq2 library will be used. First a *DESeqDataSet*-object will need to be created since that's what is needed to use the function. There are multiple techniques to normalize data but for now only normalized data from a variance stabilizing transformation will be used.

```{r normalize data}
# Create DESeqDataSet object
ddsMat <- DESeqDataSetFromMatrix(countData = myData, design = ~ 1,
                                 colData = data.frame(samples = names(myData)))
# Perform normalization
rld.dds <- vst(ddsMat)
# 'Extract' normalized values
rld <- assay(rld.dds)

# Pretty print the head of the normalized data
panderOptions("table.continues", "")
pander::pander(head(rld), split.tables = 70)
```

Normalized data has now been obtained with values that are comparable, unlike the raw read counts. What looked like differences in expression in the count data could now with the normalized data look no different at all between the groups.


<!-- (Page 09 - Creating a heatmap to see sample distances) -->
\newpage
## Creating a heatmap to see sample distances
To see what the differences in expression between groups are, the data will be visualized with a heatmap. This heatmap will show the Euclidean distances between all samples. 
To make a heatmap distance calculations need to be performed, for each combination of samples a distance metric is calculated that will be used to check for variation within the sample groups. The normalized data will be transposed and only after that can be used to calculate the sample distances.

```{r calculate sample distances and create heatmap, fig.width=5, fig.height=4}
# Calculate basic distance metric (using euclidean distance)
sampledists <- dist(t(rld))

# Convert the 'dist' object into a matrix for creating a heatmap
sampleDistMatrix <- as.matrix(sampledists)

# The annotation is an extra layer that will be plotted above the heatmap columns
annotation <- data.frame(SPL = groups)

# Set the rownames of the annotation dataframe to the sample names
rownames(annotation) <- names(myData)

# Create heatmap
pheatmap(sampleDistMatrix, main = "Euclidean Sample Distances",
         annotation_col = annotation, show_colnames = TRUE,
         clustering_distance_rows = sampledists,
         clustering_distance_cols = sampledists)
```


<!-- (Page 10 - Create second heatmap without changed clustering) -->
\newpage
The resulting heatmap is not as beautiful as desired and thus does not give that much insight in the data. This is probably mainly due to the fact that all the samples are from the exact same type of tissue and the only differentiating factor is the Sound Pressure Level the mice were exposed to. With the idea that it might be more beneficial to create a heatmap that doesn't change the clustering a second one was created.

```{r create second heatmap without changed clustering, fig.width=5, fig.height=3}
# Create second heatmap without changed clustering
pheatmap(sampleDistMatrix, main = "Sample Distances Without Clustering",
          annotation_col = annotation, show_colnames = FALSE,
          cluster_rows = FALSE, cluster_cols = FALSE)
```

In this second heatmap it can be better seen that there are trends between and within sample groups, almost creating borders between groups. Because there are a few samples with noticeable differentiating values from the other samples in their own group the first heatmap became less readable since there is only 1 factor that differs. One could also say that by looking at this heatmap sample A4 looks to be an outlier and might cause noise in statistics later on.


<!-- (Page 11 - Getting Poisson distance data) -->
\newpage
## Using Multi-Dimensional Scaling to spot clustering
Another way to see how distant samples are from each other is by performing Multi-Dimensional Scaling (MDS), this creates a 2D plot where it is the hope that distinct clusters are formed. What's different with MDS is that a slightly different distance metric is used; Poisson instead of the previously used Euclidean with the dist() function. To get the *Poisson Distance* data the 'PoiClaClu' library is used, it uses the raw count data (keep in mind the 'ddsMat' DataSet still contains raw count data, only the 'rld' matrix is normalized).

```{r getting poisson distance data}
# Note: uses the raw-count data, PoissonDistance performs normalization
# set by the 'type' parameter (uses DESeq)
dds <- assay(ddsMat)
poisd <- PoissonDistance(t(dds), type = "deseq")
# Extract the matrix with distances
samplePoisDistMatrix <- as.matrix(poisd$dd)

# Calculate the MDS and get the X- and Y-coordinates
mdsPoisData <- data.frame( cmdscale(samplePoisDistMatrix) )
# And set some better readable names for the columns
names(mdsPoisData) <- c('x_coord', 'y_coord')
mdsPoisData
```

Now the MDS coordinates used for plotting the distances using Poisson Distance are calculated they'll be plotted with the ggplot library.


<!-- (Page 12 - Create MDS plot) -->
\newpage
```{r create MDS plot}
# Separate the annotation factor (as the variable name is used as label)
coldata <- names(myData)

# Create the plot using ggplot
ggplot(mdsPoisData, aes(x_coord, y_coord, color = groups, label = coldata)) + theme_bw() +
       geom_text(size = 4) + ggtitle('Multi Dimensional Scaling') +
       labs(x = "Poisson Distance", y = "Poisson Distance")
```

The three different groups seem to cluster pretty nicely in the generated plot, there is a noticeable separation between the different SPL groups' coordinates. 

Based on all of the exploratory data analysis that was done it looks like all the samples are usable so that there does not seem to be a need to do any further data cleaning. This marks the end of this chapter and now the 'real' analysis will start.


<!-- (Page 13 - Starting chapter 4: Discovering DEGs) -->
\newpage
# Discovering Differentially Expressed Genes (DEGs)
The goal of this project and what will be focused on now is to try to discover differentially expressed genes, these are genes that show a difference in expression between sample groups. Simply said it's done by calculating the ratios for all genes between samples to determine the **fold-change** (FC) denoting the factor of change in expression between groups. Then, filter out only those genes that actually show a difference. While this might look like it immediately works, what's almost more important is that the DEGs are statistically significant.

In the chosen research the interactive gene expression analysis kit *iGEAK* was used, which is based on the R/shiny platform, so most methods can be replicated in the environment this project is being done (RStudio).

## Pre-processing
### Sample removal
After doing the exploratory data analysis there does not seem to be a direct need to remove samples from the data set. It will however be checked if removing the A4 sample groups causes a reduction noise and lower p-values thus if it results in higher overall statistical significance. 
<!--ADD TEXT WHEN THIS WAS TESTED WHAT THE FINDINGS WERE--> 

### Filtering (partially) inactive genes
An important step in pre-processing is to filter out (partially) inactive genes from the data set, this is even required for using the `edgeR` library.
This step is done manually, since removing genes can be very subjective to the experiment, it might be expected (thus important) when comparing different tissues or knockout experiments. Genes with a (very) low read count (< 5) can give a very high (artificial) FC value, when comparing two samples where one has a value of 2 and the other 11, this reads as an up-regulated gene by a factor of 5.5 while it might actually just be noise.

Since the chosen research also filtered very low signals, the same formula will be used. The researchers filtered the genes by requiring a minimum read count of 8 in at least 4 samples. The way it is done below here is that an index filter is created saying which genes have passed the criteria by marking them as TRUE or FALSE. For each gene/row it gets the sum of how many samples have 8 or more counts, if a gene has 4 or more samples with at least 8 counts then it returns it as TRUE. The index is then applied to the original data frame resulting in a new data frame with only the genes that match the criteria.

```{r Filtering (partially) inactive genes}
# Create filter index
idx <- rowSums( myData >= 8 ) >= 4
# Use the index to create the new data frame with only the genes that passed
myData.filtered <- myData[idx,]
```

To be sure the filtering was successful, a manual check was done. It can be seen that the index has TRUE and FALSE values for each gene/row, when applying the filter and checking the data frame for which genes are left with which values it all seems to have worked great. Before filtering there were **35496** genes and after filtering there are **20315** genes left. This means that more than a third, **15181** genes to be exact, were filtered out.


<!-- (Page 14/15 - FPM Normalization and the Fold Change value) -->
\newpage
### FPM Normalization
Before using other libraries another manual step will be done, the normalization of the data will be re-done with a different technique. Performing the simple method of calculating the *fragments per million mapped fragments* (FPM) value and then log2 transforming it. This does not include the gene-length, which is not known for the data set, so it can't be used anyways. FPM is calculated by dividing the count values by the total number of reads of that sample divided by 1 million. For now all genes, not only the filtered genes are used, to later see the difference of what the results look like without filtering and with.

```{r FPM normalization}
# Perform a naive FPM normalization
# Note: log transformation includes a pseudo count of 1
myData.fpm <- log2( (myData / (colSums(myData) / 1e6)) + 1 )
head(myData.fpm)
```


## The *Fold Change* value
Fold Change (FC) is a measure that describes the degree of quantity change between final and original values, it is usually given as the calculated `log2` of the case/control ratio. Ratio values > 1 indicate increased expression in the experiment in relation to the control and values between 0 and 1 indicate lower expression but can have a huge range of numbers. Using log2 values to display the ratios makes it easier to distinguish between down-regulated expression `log2(ratio) < 0` and up-regulated expression `log2(ratio) > 0`.

For example, gene A has an average expression of 30 mapped reads in the control group and 88 reads in the experiment group, the *ratio* case/control (`88/30`) is `2.93`. If the counts were reversed, the ratio would have been `30/88`, which is `0.34`. The previously calculated value of `2.93` means a *3-fold up regulation* while the `0.34` value means *3-fold down regulation* but as you can see the range of numbers is very different. For easier comparing the log2 values are used, the log2 transformed value of the ratio is calculated with `log2(ratio)`. In this example the log2 values would be `log2(2.93) = 1.55` and `log2(0.34) = -1.55`, these log2 fold changes compare much better and easier.

### Histogram showing FC
To visualize the FC ratio of *all* genes between the 105db (case) and the ambient (control) group a histogram is made. For each group the average of the log2(FPM) is calculated per gene and added as a new column to the previously created FPM normalized data frame. The FC values are then calculated for the 105dB group against the 70dB control group after which they are plotted in a histogram. Two ablines will also be shown in the histogram at -1 and 1, these indicate where the border is of which FC values could be considered DEGs.

```{r calculating FCs}
# Create new column for each group with the average of genes' log2(FPM) value's
myData.fpm$avg70dB <- rowMeans(myData.fpm[1:4])
myData.fpm$avg94dB <- rowMeans(myData.fpm[5:8])
myData.fpm$avg105dB <- rowMeans(myData.fpm[8:12])

# Calculate FC for 105dB against 70dB control group
myData.fpm$FC105dB_70dB <- myData.fpm$avg105dB - myData.fpm$avg70dB

# Plot histogram showing the FC
hist(myData.fpm$FC105dB_70dB, breaks=100, col = "cyan",
     main = "Histogram showing log2 FC values of all genes", 
     xlab = "log2 FC", ylab = "FC value Frequency")

# Add ablines indicating DEG territory (outside = DEG)
abline(v = c(-1, 1), col="red")
```

Looking at the histogram almost all genes lie within -1 and 1 (the red ablines), outside the ablines the frequency is so low that they can't be seen. However this does not mean there are not any high enough FC values outside the lines! 

An FC value by itself does not mean much, because if the variation *within* a group is very high, it could just be biological noise or a sequencing error. It needs to be determined if the observed FC is statistically significant. To determine significance, **t-test**s are performed using all the replicates of a group. These t-tests will however not be done manually but by using the Bioconductor package `edgeR`, it performs the t-tests and significance is then indicated by the resulting p-value. If a genes' p-value is below the alpha of 0.05 it is considered a statistically-significant differentially expressed gene (SDEG).

<!-- (Page 16 - Using Bioconductor Packages for DEG Analysis: Creating a design matrix) -->
\newpage
## Using Bioconductor Packages for DEG Analysis
Then the data was normalized using the `edgeRz`'s TMM (Trimmed Mean of M-values) normalization method, the resulting P-values were adjusted using the *Benjamini and Hochbergâ€™s* approach for controlling the False Discovery Rate (FDR). Genes with an adjusted P value < 0.05 and fold change > 1.5 found by edgeR were assigned as differentially expressed genes.

The goal now is to perform similar analysis as what was done in the chosen research. To perform DEG analysis on the raw count data there are a lot of packages included in Bioconductor but for this project the focus lies on `edgeR` and `DESeq2`. First the `DESeq2` package will be used and to later compare to the `edgeR` package, which will be more extensively covered because it was also used in the chosen research experiment.

### Creating a `Design Matrix`
To use any of these packages it needs to be properly specified how the samples are grouped. With more than two samples it is important to know what differences will want to be looked for. The time is not relevant for this particular experiment since all samples are taken at the same time after exposure, the main thing this project is looking for is the difference in expression caused by different sound pressure levels. Since the first group, with the 70dB SPL, is our control group nothing has to be done about the levels.

```{r creating the design matrix}
# Create a design matrix using the previously created `groups` variable
(myDesign <- model.matrix( ~ groups))
```


<!-- (Page 16 - edgeR) -->
\newpage
### edgeR
...
